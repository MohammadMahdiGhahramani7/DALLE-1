{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from DalleDecoder import DecoderOnlyTransformer\n",
        "from DallEdVAE import dVAE\n",
        "from train_transformer import train\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "import time\n",
        "import pickle\n",
        "\n",
        "\n",
        "############################ dVAE ############################\n",
        "# (I) Loading dVAE\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "inp_ch = 3\n",
        "n_hid = 256\n",
        "n_init = 128\n",
        "bpg = 2\n",
        "K = 8192\n",
        "D = 512\n",
        "Beta = 6.6\n",
        "path_to_dvae = 'path_to_pretrained_dVAE'\n",
        "\n",
        "dvae = dVAE(inp_ch, n_hid, n_init, bpg, K, D, Beta).to(device)\n",
        "dvae.load_state_dict(torch.load(path_to_dvae))\n",
        "\n",
        "######################## Data Loading ########################\n",
        "# (II) Data (img-txt pairs) to Train Transformer\n",
        "path_to_data = 'path_to_img_txt_pair_dataset'\n",
        "batch_size = 256\n",
        "\n",
        "with open(path_to_data, 'rb') as f:\n",
        "    dataset = pickle.load(f)\n",
        "\n",
        "tokenizer = get_tokenizer('basic_english')\n",
        "def create_tokens(dataset):\n",
        "  for sample in dataset:\n",
        "    yield tokenizer(sample[1][1]) \n",
        "\n",
        "vocab = build_vocab_from_iterator(create_tokens(dataset), specials=[\"<start>\"]) \n",
        "vocab.set_default_index(vocab[\"<start>\"])\n",
        "\n",
        "with open(f'vocab.pkl', 'wb') as f:\n",
        "    pickle.dump(vocab, f)\n",
        "\n",
        "text_vocab_size = len(vocab) # 16384 in the paper\n",
        "text_seq_len = 256\n",
        "\n",
        "class MyDataset(Dataset):\n",
        "    def __init__(self, dataset, dvae, tokenizer, vocab, text_seq_len, text_vocab_size, device):\n",
        "      super().__init__()\n",
        "      self.dataset = dataset\n",
        "      self.dvae = dvae\n",
        "      self.tokenizer = tokenizer\n",
        "      self.vocab = vocab\n",
        "      self.text_seq_len = text_seq_len\n",
        "      self.text_vocab_size = text_vocab_size\n",
        "      self.device = device\n",
        "\n",
        "    def _sent_padding(self, sent_vec, maxlen):\n",
        "      sent_vec = torch.tensor(sent_vec)\n",
        "      maxlen -= len(sent_vec)\n",
        "      return F.pad(sent_vec, (0, maxlen))\n",
        "\n",
        "    def text2token(self, text):\n",
        "      text_vector = self._sent_padding(self.vocab(self.tokenizer(text)), maxlen=self.text_seq_len)\n",
        "      text_range = torch.arange(self.text_seq_len) + self.text_vocab_size\n",
        "      text = torch.where(text_vector == 0, text_range, text_vector) # <pad_i> tokens\n",
        "      text = F.pad(text, (1, 0), value = 0) # add <bos>\n",
        "      return text\n",
        "\n",
        "    def __len__(self):\n",
        "      return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "      img, (file_name, txt) = self.dataset[idx]\n",
        "      txt_tokens = self.text2token(txt)\n",
        "      # txt_tokens: [257]\n",
        "      img_tokens = self.dvae.get_code_book(img.unsqueeze(0)) # inp: [1, 3, 256, 256] \n",
        "      # img_tokens: [1, 1024]\n",
        "      img_tokens = img_tokens.squeeze(0)\n",
        "      # img_tokens: [1024]\n",
        "      return (txt_tokens.to(self.device), img_tokens.to(self.device))\n",
        "\n",
        "dataset = MyDataset(dataset, dvae, tokenizer, vocab, text_seq_len, text_vocab_size, device)\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "######################## Transformer #########################\n",
        "# (III) Loading Transformer\n",
        "image_vocab_size = K # 8192\n",
        "image_seq_len = 1024\n",
        "d_model = 512\n",
        "N = 64\n",
        "heads = 64\n",
        "d_ff = 2048\n",
        "path_to_transformer = None\n",
        "\n",
        "total_len_text_vocab = text_vocab_size + text_seq_len # 16384 + 256 in the paper\n",
        "\n",
        "loss_img_weight = 7\n",
        "\n",
        "epochs = 20\n",
        "lr = 1e-3\n",
        "print_step = 10\n",
        "\n",
        "dec_only_transformer = DecoderOnlyTransformer(text_vocab_size, text_seq_len,\n",
        "                                              image_vocab_size, image_seq_len,\n",
        "                                              d_model, N, heads, d_ff).to(device)\n",
        "\n",
        "if path_to_transformer != None:\n",
        "    dec_only_transformer.load_state_dict(torch.load(path_to_transformer))\n",
        "\n",
        "optimizer = optim.Adam(dec_only_transformer.parameters(), lr=lr)\n",
        "\n",
        "########################## Training ###########################\n",
        "# (IV) Training the Transformer\n",
        "start_time = time.time()\n",
        "total_L = train(dec_only_transformer, optimizer, dataloader, epochs, text_seq_len,\n",
        "                total_len_text_vocab, loss_img_weight, print_step)\n",
        "\n",
        "end_time = time.time()\n",
        "print(f\"Training time: {end_time- start_time:.3f} seconds\")"
      ],
      "metadata": {
        "id": "qDuhikxr7EL1"
      },
      "execution_count": 2,
      "outputs": []
    }
  ]
}
