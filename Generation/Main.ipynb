{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "vHZqW87DEcWv"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from DalleDecoder import DecoderOnlyTransformer\n",
        "from DallEdVAE import dVAE\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path_to_vocab = 'path_to_vocab.pkl'\n",
        "path_to_dvae = 'path_to_pretrained_dVAE'\n",
        "path_to_transformer = 'path_to_pretrained_transformer'\n",
        "\n",
        "tokenizer = get_tokenizer('basic_english')\n",
        "\n",
        "with open(path_to_vocab, 'rb') as f:\n",
        "    vocab = pickle.load(f)\n",
        "\n",
        "text_vocab_size = len(vocab)\n",
        "text_seq_len = 256\n",
        "total_len_text_vocab = text_vocab_size + text_seq_len\n",
        "image_seq_len = 1024\n",
        "\n",
        "def sent_padding(sent_vec, maxlen):\n",
        "    sent_vec = torch.tensor(sent_vec)\n",
        "    maxlen -= len(sent_vec)\n",
        "    return F.pad(sent_vec, (0, maxlen))\n",
        "\n",
        "def text2token(text):\n",
        "    text_vector = sent_padding(vocab(tokenizer(text)), maxlen=text_seq_len)\n",
        "    text_range = torch.arange(text_seq_len) + text_vocab_size\n",
        "    text = torch.where(text_vector == 0, text_range, text_vector) # tokens\n",
        "    text = F.pad(text, (1, 0), value = 0) # add <bos>\n",
        "    return text"
      ],
      "metadata": {
        "id": "r0p6iBQfEhfD"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inp_ch = 3\n",
        "n_hid = 256\n",
        "n_init = 128\n",
        "bpg = 2\n",
        "K = 8192\n",
        "D = 512\n",
        "Beta = 6.6\n",
        "\n",
        "dvae = dVAE(inp_ch, n_hid, n_init, bpg, K, D, Beta)\n",
        "dvae.load_state_dict(torch.load(path_to_dvae))"
      ],
      "metadata": {
        "id": "k-7vUrm8S03I"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_vocab_size = 8192\n",
        "image_seq_len = 1024\n",
        "d_model = 512\n",
        "N = 64\n",
        "heads = 64\n",
        "d_ff = 2048\n",
        "\n",
        "transformer = DecoderOnlyTransformer(text_vocab_size, text_seq_len, image_vocab_size,\n",
        "                                     image_seq_len, d_model, N, heads, d_ff)\n",
        "\n",
        "transformer.load_state_dict(torch.load(path_to_transformer))"
      ],
      "metadata": {
        "id": "Yr6N5eoIK9hP"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def generation(text):\n",
        "\n",
        "    image_tokens = torch.Tensor(image_seq_len).fill_(0).to(torch.long).unsqueeze(0) # [1, 1024]\n",
        "    text_tokens = text2token(text).unsqueeze(0) # [1, 257]\n",
        "\n",
        "    for i in range(image_seq_len): # 1024\n",
        "        logits = transformer(text_tokens, image_tokens) \n",
        "        # logits: [1, 1280, 16384 + 256 + 8192]\n",
        "        logits = logits[:, text_seq_len + i, :] \n",
        "        # logits: [1, 1, 16384 + 256 + 8192]\n",
        "        next_img_token = torch.argmax(logits, dim=-1)\n",
        "        assert next_img_token - total_len_text_vocab >= 0, \"ERROR, BAD TRANSFORMER!\"\n",
        "        image_tokens[:, i] = next_img_token - total_len_text_vocab # offset reverse\n",
        "\n",
        "    one_hot = torch.zeros(image_seq_len, K)\n",
        "    one_hot.scatter_(1, image_tokens, 1)\n",
        "    quantized = torch.matmul(one_hot, dvae.QNTZ.embedding.weight).reshape((1, 32, 32, D))\n",
        "    z_q_x = quantized.permute(0, 3, 1, 2)\n",
        "    generated_image = dvae.decoder(z_q_x).squeeze(0)\n",
        "\n",
        "    return generated_image\n",
        "\n",
        "input_text = 'a dog in the water'\n",
        "generated_image = generation(input_text)\n",
        "\n",
        "plt.imshow(generated_image.permute(1, 2, 0))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "CrWuVs3yFe7d"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
